<?xml version="1.0" ?>

<!--  
 This is the Solr schema file example for Terminology Recognition content indexing engine.
 -->
<schema name="jate" version="2.0">
    <!-- Valid attributes for fields:
      name: mandatory - the name for the field
      type: mandatory - the name of a field type from the
        <types> fieldType section
      indexed: true if this field should be indexed (searchable or sortable)
      stored: true if this field should be retrievable
      docValues: true if this field should have doc values. Doc values are
        useful for faceting, grouping, sorting and function queries. Although not
        required, doc values will make the index faster to load, more
        NRT-friendly and more memory-efficient. They however come with some
        limitations: they are currently only supported by StrField, UUIDField
        and all Trie*Fields, and depending on the field type, they might
        require the field to be single-valued, be required or have a default
        value (check the documentation of the field type you're interested in
        for more information)
      multiValued: true if this field may contain multiple values per document
      omitNorms: (expert) set to true to omit the norms associated with
        this field (this disables length normalization and index-time
        boosting for the field, and saves some memory).  Only full-text
        fields or fields that need an index-time boost need norms.
        Norms are omitted for primitive (non-analyzed) types by default.
      termVectors: [false] set to true to store the term vector for a
        given field.
        When using MoreLikeThis, fields used for similarity should be
        stored for best performance.
      termPositions: Store position information with the term vector.
        This will increase storage costs.
      termOffsets: Store offset information with the term vector. This
        will increase storage costs.
      termPayloads: Store payload information with the term vector. This
        will increase storage costs.
      required: The field is required.  It will throw an error if the
        value does not exist
      default: a value that should be used if no value is specified
        when adding a document.
    -->
    <types>
        <fieldtype name="string" class="solr.StrField" sortMissingLast="true" omitNorms="true"/>

        <fieldType name="int" class="solr.TrieIntField" precisionStep="0" positionIncrementGap="0"/>
        <fieldType name="float" class="solr.TrieFloatField" precisionStep="0" positionIncrementGap="0"/>
        <fieldType name="long" class="solr.TrieLongField" precisionStep="0" positionIncrementGap="0"/>
        <fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" positionIncrementGap="0"/>
        <!-- boolean type: "true" or "false" -->
        <fieldType name="boolean" class="solr.BoolField" sortMissingLast="true"/>

        <!-- This will stop your Solr from crashing when Tika index fields that Solr don't know of. -->
        <fieldtype name="ignored" stored="false" indexed="false" multiValued="true" class="solr.StrField"/>

        <!-- The format for this date field is of the form 1995-12-31T23:59:59Z, and
             is a more restricted form of the canonical representation of dateTime
             http://www.w3.org/TR/xmlschema-2/#dateTime
             The trailing "Z" designates UTC time and is mandatory.
             Optional fractional seconds are allowed: 1995-12-31T23:59:59.999Z
             All other components are mandatory.

             Expressions can also be used to denote calculations that should be
             performed relative to "NOW" to determine the value, ie...

                   NOW/HOUR
                      ... Round to the start of the current hour
                   NOW-1DAY
                      ... Exactly 1 day prior to now
                   NOW/DAY+6MONTHS+3DAYS
                      ... 6 months and 3 days in the future from the start of
                          the current day

             Consult the TrieDateField javadocs for more information.

             Note: For faster range queries, consider the tdate type
        -->
        <fieldType name="date" class="solr.TrieDateField" precisionStep="0" positionIncrementGap="0"/>
        <!--<fieldType name="pdate" class="solr.DateField" sortMissingLast="true" />-->
        <fieldType name="pdate" class="solr.TrieDateField" precisionStep="6" positionIncrementGap="0"/>


        <field name="payloads" type="payloads" indexed="true" stored="true"/>

        <!-- A general text field that has reasonable, generic
         cross-language defaults: it tokenizes with StandardTokenizer,
        removes stop words from case-insensitive "stopwords.txt"
        (empty by default), and down cases.  At query time only, it
        also applies synonyms. -->

        <fieldType name="text_general" class="solr.TextField" positionIncrementGap="100">
            <analyzer type="index">
                <tokenizer class="solr.StandardTokenizerFactory"/>
                <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>
                <!-- in this example, we will only use synonyms at query time <filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/> -->
                <filter class="solr.LowerCaseFilterFactory"/>
            </analyzer>
            <analyzer type="query">
                <tokenizer class="solr.StandardTokenizerFactory"/>
                <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>
                <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
                <filter class="solr.LowerCaseFilterFactory"/>
            </analyzer>
        </fieldType>
        <!-- A text field with defaults appropriate for English: it
         tokenizes with StandardTokenizer, removes English stop words
         (lang/stopwords_en.txt), down cases, protects words from protwords.txt, and
         finally applies Porter's stemming.  The query time analyzer
         also applies synonyms from synonyms.txt. -->
        <fieldType name="text_en" class="solr.TextField" positionIncrementGap="100">
            <analyzer type="index">
                <tokenizer class="solr.StandardTokenizerFactory"/>
                <!-- in this example, we will only use synonyms at query time
                <filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/>
                -->
                <!-- Case insensitive stop word removal.
                -->
                <filter class="solr.StopFilterFactory"
                        ignoreCase="true"
                        words="stopwords.txt"/>

                <filter class="solr.LowerCaseFilterFactory"/>
                <filter class="solr.EnglishPossessiveFilterFactory"/>
                <filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/>
                <!-- Optionally you may want to use this less aggressive stemmer instead of PorterStemFilterFactory:
                    <filter class="solr.EnglishMinimalStemFilterFactory"/>
                -->
                <filter class="solr.PorterStemFilterFactory"/>
            </analyzer>
            <analyzer type="query">
                <tokenizer class="solr.StandardTokenizerFactory"/>
                <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
                <filter class="solr.StopFilterFactory"
                        ignoreCase="true"
                        words="stopwords.txt"/>
                <filter class="solr.LowerCaseFilterFactory"/>
                <filter class="solr.EnglishPossessiveFilterFactory"/>
                <filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/>
                <!-- Optionally you may want to use this less aggressive stemmer instead of PorterStemFilterFactory:
                    <filter class="solr.EnglishMinimalStemFilterFactory"/>
                -->
                <filter class="solr.PorterStemFilterFactory"/>
            </analyzer>
        </fieldType>        

        <fieldType name="payloads" stored="false" indexed="true" class="solr.TextField">
            <analyzer>
                <tokenizer class="solr.WhitespaceTokenizerFactory"/>
                <!--
                The DelimitedPayloadTokenFilter can put payloads on tokens... for example,
                a token of "foo|1.4"  would be indexed as "foo" with a payload of 1.4f
                Attributes of the DelimitedPayloadTokenFilterFactory :
                 "delimiter" - a one character delimiter. Default is | (pipe)
             "encoder" - how to encode the following value into a playload
                float -> org.apache.lucene.analysis.payloads.FloatEncoder,
                integer -> o.a.l.a.p.IntegerEncoder
                identity -> o.a.l.a.p.IdentityEncoder
                    Fully Qualified class name implementing PayloadEncoder, Encoder must have a no arg constructor.
                 -->
                <filter class="solr.DelimitedPayloadTokenFilterFactory" encoder="float"/>
            </analyzer>
        </fieldType>

        <!--
         Numeric field types that index each value at various levels of precision
         to accelerate range queries when the number of values between the range
         endpoints is large. See the javadoc for NumericRangeQuery for internal
         implementation details.

         Smaller precisionStep values (specified in bits) will lead to more tokens
         indexed per value, slightly larger index size, and faster range queries.
         A precisionStep of 0 disables indexing at different precision levels.
        -->
        <fieldType name="tint" class="solr.TrieIntField" precisionStep="8" positionIncrementGap="0"/>
        <fieldType name="tfloat" class="solr.TrieFloatField" precisionStep="8" positionIncrementGap="0"/>
        <fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" positionIncrementGap="0"/>
        <fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" positionIncrementGap="0"/>

        <!-- The "RandomSortField" is not used to store or search any
             data.  You can declare fields of this type it in your schema
             to generate pseudo-random orderings of your docs for sorting
             or function purposes.  The ordering is generated based on the field
             name and the version of the index. As long as the index version
             remains unchanged, and the same field name is reused,
             the ordering of the docs will be consistent.
             If you want different psuedo-random orderings of documents,
             for the same version of the index, use a dynamicField and
             change the field name in the request.
         -->
        <fieldType name="random" class="solr.RandomSortField" indexed="true"/>

        <!-- ###################### JATE Start #############################-->
        <!-- A required field type for terminology recognition. Any text processed by this field type will be transformed 
			into ngram tokens (not candidates). The text field associated with this field type should store the necessary 
			information including start/end offsets and term vector.
			
            These auto-generated n-gram tokens from raw text are required by ranking algorithms for the lookup of frequency 
			information of candidate terms.

            WARNING: YOU MUST ENSURE YOUR fieldType DEFINITION FOR YOUR CANDIDATE TERM field (i.e., jate_text_2_terms in 
			default setting) USE THE SAME tokenizer AND SETS OF (PRE/POST) filters. OTHERWISE, IT IS LIKELY THAT SOME 
			CANDIDATE TERMS WILL NOT HAVE CORRECT INFORMATION EXTRACTED DUE TO MISSING NECESSARY FREQUENCY INFORMATION.

			Default config setting options:
			tokenisation:
			    1) <tokenizer class="solr.StandardTokenizerFactory" />
			    2) <filter class="org.apache.lucene.analysis.jate.OpenNLPPOSTaggerFactory"
                        posTaggerClass="uk.ac.shef.dcs.jate.nlp.opennlp.POSTaggerOpenNLP"
                        posTaggerModel="en-pos-maxent.bin"/>
            term stemming/Lemmatisation:
                1) <filter class="solr.EnglishMinimalStemFilterFactory"/>
                2) <filter class="org.apache.lucene.analysis.jate.EnglishLemmatisationFilterFactory"
                    lemmaResourceDir="lemmatiser"/>
            term ngram generator:
                1) <filter class="org.apache.lucene.analysis.jate.ComplexShingleFilterFactory" .../>
                2) <filter class="solr.ShingleFilterFactory" minShingleSize="2" maxShingleSize="6"
						outputUnigrams="true" outputUnigramsIfNoShingles="false" tokenSeparator=" "/>
            -->
        <fieldType name="jate_text_2_ngrams" class="solr.TextField" positionIncrementGap="100">
            <analyzer type="index">
                <!-- Solr standard filter to cleanse html unicode characters -->
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u0027" replacement=" ' " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u000a" replacement=" \\n " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u0009" replacement=" \\t " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u0008" replacement=" \\b " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u000d" replacement=" \\r " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u000c" replacement=" \\f " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u0022" replacement=" &quot; " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u005c" replacement=" \\ " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u003c" replacement=" &lt; " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u003e" replacement=" &gt; " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u003d" replacement=" = " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u0026" replacement=" &amp; " />
                <!-- Solr standard filter to cleanse html entities -->
                <charFilter class="solr.HTMLStripCharFilterFactory"/>
                <!--<tokenizer class="solr.StandardTokenizerFactory" />-->
                <!-- A tokenizer based on OpenNLP 
			         -sentenceModel: must point to the statistical sentence splitting model trained
			                       for OpenNLP. This must be placed in the same folder as this schema
			         -tokenizerModel: must point to the statistical tokenization model trained for
			                       OpenNLP. This must be placed in the same folder as this schema file
			    	-->
                <tokenizer class="org.apache.lucene.analysis.jate.OpenNLPTokenizerFactory"
                           sentenceModel="en-sent.bin"
                           tokenizerModel="en-token.bin"/>
                <!-- Asciifolding that converts alphabetic, numeric, and symbolic Unicode characters to ASCII characters-->
                <filter class="solr.ASCIIFoldingFilterFactory"/>
                <!-- A PoS tagger based on OpenNLP 
			         -posTaggerClass: must point to class to be used for tagging. This must be an implementing
			         	class of uk.ac.shef.dcs.jate.nlp.POSTagger. 
			         -posTaggerModel: must point to a binary file trained for OpenNLP tagging. This must be placed 
			         	in the same folder as this schema
			         -->
                <filter class="org.apache.lucene.analysis.jate.OpenNLPPOSTaggerFactory"
                        posTaggerClass="uk.ac.shef.dcs.jate.nlp.opennlp.POSTaggerOpenNLP"
                        posTaggerModel="en-pos-maxent.bin"/>
                <!-- A shingle generator based on that distributed with Solr, but enhanced to cleanse generated shingles
					 The following parameters are shared by any implementing classes of its super-class 
					 org.apache.lucene.analysis.jate.MWEFilterFactory  
					 -minTokens: minimum number of tokens/words that must be present in a shingle (inclusive)
					 -maxTokens: maximum number of tokens/words (inclusive)
					 -maxCharLength: maximum number of characters of the shingle
					 -minCharLength: minimum number of characters of the shingle
			         -removeLeadingStopWords: if true, shingle "the big cat and" => "big cat and", if "the" is a stopword
			         -removeTrailingStopWords: if true, shingle "the big cat and" => "the big cat", if "and" is a stopword
					 -removeLeadingSymbolicTokens: if true, shingle "- the cat -" => "the cat -"
					 -removeTrailingSymbolicTokens: if true, shingle "- the cat -" => "- the cat"
					 -stripAnySymbolChars: if true, shingle "-the-cat-" => "the cat"
					 -stripLeadingSymbolChars: if true, shingle "-the-cat-" => "the-cat-"
					 -stripTrailingSymbolChars: if true, shingle "-the-cat-" => "-the-cat"
					 -stopWords: a list of stop words in a file. Must be one word per line, and file must be placed in 
					 	the same folder as this schema
					 -stopWordsIgnoreCase: if true, checking for stop words will be case-insensitive

			         The following parameters are specific to this shingle filter and inherit the original shingle filter
			         distributed with solr
			         -outputUnigrams
			         -outputUnigramsIfNoShingles
			         -tokenSeparator
			         -->					
                <filter class="org.apache.lucene.analysis.jate.ComplexShingleFilterFactory" minTokens="2" maxTokens="5"
                        maxCharLength="40" minCharLength="2" removeLeadingStopWords="true"
                        removeTrailingStopWords="true" removeLeadingSymbolicTokens="true"
                        removeTrailingSymbolicTokens="true"
                        stripAnySymbolChars="false"
                        stripLeadingSymbolChars="true" stripTrailingSymbolChars="true"
                        stopWords="stopwords.txt" stopWordsIgnoreCase="true"
                        outputUnigrams="true" outputUnigramsIfNoShingles="false" tokenSeparator=" "/>
                <filter class="solr.LowerCaseFilterFactory"/>
                <!--<filter class="solr.EnglishMinimalStemFilterFactory"/>-->
                <!-- An English lemmatiser 
					-lemmaResourceDir: must be a folder placed in the same folder as this schema, containing the resource
						files required by the Dragon Tool Kit English lemmatiser
					-->		
                <filter class="org.apache.lucene.analysis.jate.EnglishLemmatisationFilterFactory"
                    lemmaResourceDir="lemmatiser"/>
                <!-- standard Solr stop words filter -->
                <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>
            </analyzer>
        </fieldType>

        <!-- Three options of term candidate extraction example as below.
        Please enable preferred one to perform candidate extraction at index-time -->

        <!-- A field type for terminology recognition. Any text put into this type of field is
            processed to extract CANDIDATE TERMS to be considered by JATE.

            This is an example fieldType definition that extracts phrases matching certain Part-
            of-Speech patterns.

            WARNING: YOU MUST ENSURE YOUR fieldType DEFINITIONs FOR YOUR n-gram information FIELD
            (default=jate_text_2_ngrams) AND THAT FOR YOUR word FIELD (default=jate_text_2_words)
            USE THE SAME SET OF filters
            -->

        <!--a configuration for PoS based candidate extraction-->
        <fieldType name="jate_text_2_terms" class="solr.TextField" positionIncrementGap="100">
            <analyzer type="index">
                <!-- Solr standard filter to cleanse html unicode characters -->
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u0027" replacement=" ' " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u000a" replacement=" \\n " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u0009" replacement=" \\t " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u0008" replacement=" \\b " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u000d" replacement=" \\r " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u000c" replacement=" \\f " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u0022" replacement=" &quot; " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u005c" replacement=" \\ " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u003c" replacement=" &lt; " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u003e" replacement=" &gt; " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u003d" replacement=" = " />
                <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="\\u0026" replacement=" &amp; " />
                <!-- Solr standard filter to cleanse html entities -->
                <charFilter class="solr.HTMLStripCharFilterFactory"/>
                <!--<tokenizer class="solr.StandardTokenizerFactory" />-->
                <tokenizer class="org.apache.lucene.analysis.jate.OpenNLPTokenizerFactory"
						   sentenceModel="en-sent.bin"
						   tokenizerModel="en-token.bin"/>
  			    <!-- Asciifolding that converts alphabetic, numeric, and symbolic Unicode characters to ASCII characters-->
                <filter class="solr.ASCIIFoldingFilterFactory"/>
                <!-- A PoS tagger based on OpenNLP 
			         -posTaggerClass: must point to class to be used for tagging. This must be an implementing
			         	class of uk.ac.shef.dcs.jate.nlp.POSTagger. 
			         -posTaggerModel: must point to a binary file trained for OpenNLP tagging. This must be placed 
			         	in the same folder as this schema
			         -->
                <filter class="org.apache.lucene.analysis.jate.OpenNLPPOSTaggerFactory"
                        posTaggerClass="uk.ac.shef.dcs.jate.nlp.opennlp.POSTaggerOpenNLP"
                        posTaggerModel="en-pos-maxent.bin"/>
                <!-- A term candidate generator based on PoS sequence patterns
					 The following parameters are shared by any implementing classes of its super-class 
					 org.apache.lucene.analysis.jate.MWEFilterFactory  
					 -minTokens: minimum number of tokens/words that must be present in a candidate (inclusive)
					 -maxTokens: maximum number of tokens/words (inclusive)
					 -maxCharLength: maximum number of characters of the candidate
					 -minCharLength: minimum number of characters of the candidate
			         -removeLeadingStopWords: if true, candidate "the big cat and" => "big cat and", if "the" is a stopword
			         -removeTrailingStopWords: if true, candidate "the big cat and" => "the big cat", if "and" is a stopword
					 -removeLeadingSymbolicTokens: if true, candidate "- the cat -" => "the cat -"
					 -removeTrailingSymbolicTokens: if true, candidate "- the cat -" => "- the cat"
					 -stripAnySymbolChars: if true, candidate "-the-cat-" => "the cat"
					 -stripLeadingSymbolChars: if true, candidate "-the-cat-" => "the-cat-"
					 -stripTrailingSymbolChars: if true, candidate "-the-cat-" => "-the-cat"
					 -stopWords: a list of stop words in a file. Must be one word per line, and file must be placed in 
					 	the same folder as this schema
					 -stopWordsIgnoreCase: if true, checking for stop words will be case-insensitive

			         The following parameters are specific to this filter			         
			         -patterns: point to a file (must be in the same folder as this schema) listing the PoS sequence patterns.
						One pattern per line, and must be written as [key][\t][value], where key could be any name you give to
						the pattern, and value is the actual regular expression pattern
			         -->			
                <filter class="org.apache.lucene.analysis.jate.OpenNLPRegexChunkerFactory"
                        patterns="wind.patterns"
                        minTokens="1" maxTokens="4"
                        maxCharLength="40" minCharLength="3" removeLeadingStopWords="true"
                        removeTrailingStopWords="true" removeLeadingSymbolicTokens="true"
                        removeTrailingSymbolicTokens="true"
                        stripAnySymbolChars="false"
                        stripLeadingSymbolChars="true" stripTrailingSymbolChars="true"
                        stopWords="stopwords.txt" stopWordsIgnoreCase="true"/>
                <!--filter class="org.apache.lucene.analysis.jate.OpenNLPNounPhraseFilterFactory"
                        chunkerModel="en-chunker.bin"
                        minTokens="1" maxTokens="5"
                        maxCharLength="40" minCharLength="2" removeLeadingStopWords="true"
                        removeTrailingStopWords="true" removeLeadingSymbolicTokens="true"
                        removeTrailingSymbolicTokens="true"
                        stripAnySymbolChars="false"
                        stripLeadingSymbolChars="true" stripTrailingSymbolChars="true"
                        stopWords="stopwords.txt" stopWordsIgnoreCase="true"/-->
                <filter class="solr.LowerCaseFilterFactory" />
                <!--<filter class="solr.EnglishMinimalStemFilterFactory"/>-->
                <!-- An English lemmatiser 
					-lemmaResourceDir: must be a folder placed in the same folder as this schema, containing the resource
						files required by the Dragon Tool Kit English lemmatiser
					-->		
                <filter class="org.apache.lucene.analysis.jate.EnglishLemmatisationFilterFactory"
                        lemmaResourceDir="lemmatiser"/>
                <!-- standard Solr stop words filter -->
                <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>
		<filter class="org.apache.lucene.analysis.jate.PatternBasedStopFilterFactory" removeDigits="false" patternFile="stoppatterns.txt"/>
            </analyzer>
        </fieldType>        
        <!-- ###################### JATE End #############################-->
    </types>


    <fields>
        <!-- field names should consist of alphanumeric or underscore characters only and
          not start with a digit.  This is not currently strictly enforced,
          but other field names will not have first class support from all components
          and back compatibility is not guaranteed.  Names with both leading and
          trailing underscores (e.g. _version_) are reserved.
        -->
        <field name="_version_" type="long" indexed="true" stored="true" multiValued="false"/>
        <field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false"/>
        <field name="path" type="string" indexed="true" stored="true" required="false" multiValued="false"/>
        <!-- catchall field, containing all other searchable text fields (implemented
        via copyField further on in this schema  -->
        <field name="text" type="text_general" indexed="true" stored="true" multiValued="true"/>
        <!-- The entire document text from which candidate terms are to be extracted.
            Text will be processed by JATE to extract candidate terms. A vector is created.
            NOTE: This field is not indexed by default, since it is also copied to "text"
            using copyField below. This is to save space. Use this field for returning and
            highlighting document content. Use the "text" field to search the content. -->

        <!--##################### JATE Start ######################### -->
        <!-- Field to index text with n-gram tokens. These are used as a field to lookup information
          including frequency, offsets, etc. for candidate terms from the candidate term's field
          (default=jate_cterms). Must be indexed, termVectors and termOffsets set to true-->
        <field name="jate_ngraminfo" type="jate_text_2_ngrams" indexed="true" stored="false" multiValued="false"
               termVectors="true" termPositions="true" termOffsets="true"
               termPayloads="true"/>
        <!-- Field to index text with candidate terms. Must be indexed, and termVectors set to true-->
        <field name="jate_cterms" type="jate_text_2_terms" indexed="true" stored="false" multiValued="false"
               termVectors="true"/>

        <!-- Field to index and store filtered and weighted candidate terms.
            Set omitNorms to false to enable index-time boosting for the field.
            Note this will increase the use of memory.

            see: https://wiki.apache.org/solr/SchemaXml
        -->
        <field name="jate_domain_terms" type="string" indexed="true" stored="true" omitNorms="true" required="false" multiValued="true"/>

        <!--######################## JATE End ########################## -->

        <!-- Dynamic field definitions allow using convention over configuration
        for fields via the specification of patterns to match field names.
        EXAMPLE:  name="*_i" will match any field ending in _i (like myid_i, z_i)
        RESTRICTION: the glob-like pattern in the name attribute must have
        a "*" only at the start or the end.  -->

        <!-- OPTIONAL: a jate_terms_f* text records text from a particular position (similar to the 'field' concept
             in SOLR) of a document, e.g., title, links, first paragraph etc. Such positions are defined and required
             by each individual ATR algorithm.
             These positions are processed by JATE to extract and store candidate terms. They are not indexed for each.
             They only provide a way to trace the part of a document that a candidate term is found. Such
             information can be used by some ATR algorithms. However most ATR algorithms do not
             use such information. -->
        <dynamicField name="jate_cterms_f*" type="jate_text_2_terms" indexed="false" stored="true"/>

        <dynamicField name="*_i" type="int" indexed="true" stored="true"/>
        <dynamicField name="*_is" type="int" indexed="true" stored="true" multiValued="true"/>
        <dynamicField name="*_s" type="string" indexed="true" stored="true"/>
        <dynamicField name="*_ss" type="string" indexed="true" stored="true" multiValued="true"/>
        <dynamicField name="*_l" type="long" indexed="true" stored="true"/>
        <dynamicField name="*_ls" type="long" indexed="true" stored="true" multiValued="true"/>
        <dynamicField name="*_t" type="text_general" indexed="true" stored="true"/>
        <dynamicField name="*_txt" type="text_general" indexed="true" stored="true" multiValued="true"/>
        <dynamicField name="*_en" type="text_en" indexed="true" stored="true" multiValued="true"/>
        <dynamicField name="*_b" type="boolean" indexed="true" stored="true"/>
        <dynamicField name="*_bs" type="boolean" indexed="true" stored="true" multiValued="true"/>
        <dynamicField name="*_f" type="float" indexed="true" stored="true"/>
        <dynamicField name="*_fs" type="float" indexed="true" stored="true" multiValued="true"/>
        <dynamicField name="*_d" type="double" indexed="true" stored="true"/>
        <dynamicField name="*_ds" type="double" indexed="true" stored="true" multiValued="true"/>
        <dynamicField name="ignored_*" type="ignored" multiValued="true"/>
        <dynamicField name="random_*" type="random"/>

        <!-- copyField commands copy one field to another at the time a document
         is added to the index.  It's used either to index the same field differently,
         or to add multiple fields to the same field for easier/faster searching.
          <copyField source="text" dest="jate_cterms" />
          <copyField source="text" dest="jate_ngraminfo" />
          -->
    </fields>

    <uniqueKey>id</uniqueKey>

    <defaultSearchField>text</defaultSearchField>

    <solrQueryParser defaultOperator="OR"/>
</schema>
